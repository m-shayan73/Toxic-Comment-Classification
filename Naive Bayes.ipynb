{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ov2eXe7iWO8i",
        "outputId": "02ee40e3-db25-4167-cff3-7bce63581184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "# from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "torch.set_default_device('cuda')\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DoUOzZvoWO8m"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"/content/drive/MyDrive/ML_Project/train.csv\")\n",
        "test_x = pd.read_csv(\"/content/drive/MyDrive/ML_Project/test.csv\")\n",
        "test_y = pd.read_csv(\"/content/drive/MyDrive/ML_Project/test_labels.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ir7YKXJWgIh2"
      },
      "outputs": [],
      "source": [
        "def lowercase(txt):\n",
        "\n",
        "    return txt.lower()\n",
        "\n",
        "def remove_punctuation(txt):\n",
        "\n",
        "    return re.sub(r\"[^\\w\\s\\d]\", \"\", txt)\n",
        "\n",
        "def remove_numbers(txt):\n",
        "\n",
        "    return re.sub(r\"\\d\", \"\", txt)\n",
        "\n",
        "def normalize_sentence(txt):\n",
        "    '''\n",
        "    Aggregates all the above functions to normalize/clean a sentence\n",
        "    '''\n",
        "    txt = lowercase(txt)\n",
        "    txt = remove_punctuation(txt)\n",
        "    txt = remove_stopwords(txt)\n",
        "    txt = remove_numbers(txt)\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkB1eICsMcP6"
      },
      "source": [
        "The next step was to clean up the text. This meant taking out common words like \"the\" and \"and\" (these are called stopwords), and also removing any punctuation and numbers. Doing this helped to make the important words stand out more, which is important for correctly sorting and understanding the text.\n",
        "We split our data into two sections. The first section, called 'test_x', had the texts that we wanted our model to sort. The second section, 'test_y', had the right categories for these texts. We did the same thing with our training data. We divided it into 'train_x' (the texts for training) and 'train_y' (the matching categories)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9j9a86YNWO8n",
        "outputId": "7fe5cb7e-8ef7-4769-9f39-307b4331b4d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(63978, 9)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drop the rows whose labels are not available\n",
        "\n",
        "test_data = pd.concat([test_x, test_y], axis=1)\n",
        "test_data = test_data[(test_data['toxic'] != -1) & (test_data['severe_toxic'] != -1) & (test_data['obscene'] != -1) & (test_data['threat'] != -1) & (test_data['insult'] != -1) & (test_data['identity_hate'] != -1)]\n",
        "test_data['comment_text']=test_data['comment_text'].apply(normalize_sentence)\n",
        "\n",
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1875yLsSWO8o",
        "outputId": "2393a1d6-a23e-484b-cf0d-5194e009fb54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test_x (63978,)  ;  test_y (63978, 6)  ;  test_labels (63978,)\n"
          ]
        }
      ],
      "source": [
        "# Seperate test_x and test_y from test_data\n",
        "\n",
        "test_x = np.array(test_data[\"comment_text\"])\n",
        "test_y = np.array(test_data[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n",
        "test_labels = np.argmax(test_y, axis=1)\n",
        "\n",
        "\n",
        "print(f\"test_x {test_x.shape}  ;  test_y {test_y.shape}  ;  test_labels {test_labels.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iwUvcGZWO8o",
        "outputId": "2115ec90-4337-48a0-98d0-44b925b33b76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_x (159571,)  ;  train_y (159571, 6)\n"
          ]
        }
      ],
      "source": [
        "# Seperate train_x and train_y from train_data\n",
        "\n",
        "train_data = train_data[(train_data['toxic'] != -1) & (train_data['severe_toxic'] != -1) & (train_data['obscene'] != -1) & (train_data['threat'] != -1) & (train_data['insult'] != -1) & (train_data['identity_hate'] != -1)]\n",
        "\n",
        "train_data['comment_text']=train_data['comment_text'].apply(normalize_sentence)\n",
        "\n",
        "train_data_x = np.array(train_data[\"comment_text\"])\n",
        "train_data_y = np.array(train_data[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n",
        "\n",
        "print(f\"train_x {train_data_x.shape}  ;  train_y {train_data_y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEeZsHYCM8XZ"
      },
      "source": [
        "In the training part, we used a tool called CountVectorizer. It keeps track of how many times each word appears in the text. We paired this tool with a MultiOutputClassifier that has a MultinomialNB classifier. This mix works well for texts that can fit into more than one category.\n",
        "\n",
        "We also made a different classifier using the TF-IDF Vectorizer. This tool is useful because it does two things: it counts words in the text and finds out which words are most important. For instance, common words like \"is\" or \"of\" might be used a lot, but they don't always tell us much about the text. The TF-IDF Vectorizer helps us pay attention to rarer but more telling words. We combined this with a MultiOutputClassifier that also has a MultinomialNB classifier.\n",
        "\n",
        "Our model's main component is the Naive Bayes classifier. This approach is based on the belief that every word in the text independently influences the likelihood of the text being assigned to a specific category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATUF0Dp4bLHM",
        "outputId": "0fa27f69-11c8-4ade-abf8-0a3b3765cb4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Count Vectorizer:\n",
            "\n",
            "Accuracy 0.8796461283566226\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.64      0.61      6090\n",
            "           1       0.15      0.44      0.23       367\n",
            "           2       0.57      0.58      0.57      3691\n",
            "           3       0.01      0.02      0.02       211\n",
            "           4       0.51      0.49      0.50      3427\n",
            "           5       0.15      0.16      0.16       712\n",
            "\n",
            "   micro avg       0.50      0.55      0.53     14498\n",
            "   macro avg       0.33      0.39      0.35     14498\n",
            "weighted avg       0.52      0.55      0.53     14498\n",
            " samples avg       0.05      0.05      0.05     14498\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TF-IDF Vectrization:\n",
            "\n",
            "Accuracy: 0.9032323611241364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.20      0.32      6090\n",
            "           1       0.00      0.00      0.00       367\n",
            "           2       0.97      0.11      0.19      3691\n",
            "           3       0.00      0.00      0.00       211\n",
            "           4       0.93      0.04      0.08      3427\n",
            "           5       0.00      0.00      0.00       712\n",
            "\n",
            "   micro avg       0.93      0.12      0.21     14498\n",
            "   macro avg       0.47      0.06      0.10     14498\n",
            "weighted avg       0.85      0.12      0.20     14498\n",
            " samples avg       0.02      0.01      0.01     14498\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "\n",
        "#using count\n",
        "naive_bayes = Pipeline([\n",
        "    ('count', CountVectorizer()),\n",
        "    ('clf', MultiOutputClassifier(MultinomialNB()))\n",
        "])\n",
        "naive_bayes.fit(train_data_x, train_data_y)\n",
        "\n",
        "\n",
        "y_pred = naive_bayes.predict(test_x)\n",
        "print(\"Using Count Vectorizer:\\n\")\n",
        "print(\"Accuracy\",accuracy_score(test_y, y_pred))\n",
        "print(classification_report(test_y,y_pred))\n",
        "\n",
        "\n",
        "#using tfidf\n",
        "naive_bayes_2 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', MultiOutputClassifier(MultinomialNB()))\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "naive_bayes_2.fit(train_data_x, train_data_y)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = naive_bayes_2.predict(test_x)\n",
        "\n",
        "# Print the accuracy and classification report\n",
        "print(\"Using TF-IDF Vectrization:\\n\")\n",
        "print(\"Accuracy:\", accuracy_score(test_y, y_pred))\n",
        "print(classification_report(test_y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B_lgA90NXae"
      },
      "source": [
        "For Count Vectorization, model is doing quite well with an accuracy of about 88%, meaning it usually makes the right choice about whether to assign a label or not. However, this figure doesn't give us the full picture. In situations where there are many labels to identify, the overall accuracy may not accurately reflect the model's performance on each individual label, particularly when some labels are far more frequent than others. For labels 0, 2, and 4, the model shows decent precision, recall, and F1-scores, indicating it performs acceptably in these categories. On the other hand, labels 1, 3, and 5 have lower precision and recall, leading to poor F1-scores. This suggests that the model has difficulty making correct predictions for these labels, often either missing them (leading to false negatives) or mistakenly applying them (resulting in false positives).\n",
        "\n",
        "For the TF-IDF EVctorization, the model is pretty accurate, getting it right about 90.32% of the time when assigning a label to a sample. But this percentage doesn't give us the full story, particularly when dealing with an unbalanced dataset that has many label types. The precision, recall, and F1-scores for each label are quite low. Specifically, for labels 1, 3, and 5, these scores are zero, showing the model fails to correctly identify these labels. For labels 0, 2, and 4, although the precision is somewhat high, the recall and F1-scores are very low, indicating the model is cautious and only picks these labels in very clear situations, which leads to a lot of missed labels (false negatives).\n",
        "\n",
        "The two methods we use for organizing words in text, TF-IDF and Count Vectorization, have different impacts on the results. TF-IDF gives more attention to rare words, which could lead it to overlook important common words. This might explain why it's not as good at correctly identifying every class. Count Vectorization, on the other hand, seems to do a better job with various types of classes, showing more even results. While TF-IDF is usually more accurate overall, it falls short in how it identifies each specific class, especially in recognizing every example in a class."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
