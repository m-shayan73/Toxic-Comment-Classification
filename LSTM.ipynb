{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d95554e",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e99b23",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The LSTM (Long Short-Term Memory) model is a recurrent neural network architecture designed for sequential data processing. It is specifically chosen for toxicity classification due to its ability to capture sequential dependencies in text data. Here's an expanded explanation:\n",
    "\n",
    "#### 1. Sequential Data Processing:\n",
    "\n",
    "- **Nature of Text Data:** Toxicity classification tasks involve analyzing text data where the order and context of words matter. Traditional machine learning models struggle to capture the sequential nature of language, making them less effective for tasks like toxicity detection in sentences.\n",
    "\n",
    "- **Sequential Dependencies:** In sentences, the meaning of a word can depend on the words that precede and follow it. LSTMs are designed to address this challenge by capturing long-term dependencies in sequences, making them well-suited for tasks where context is crucial.\n",
    "\n",
    "#### 2. Overcoming Limitations of Regular RNNs:\n",
    "\n",
    "- **Vanishing Gradient Problem:** Regular RNNs suffer from the vanishing gradient problem, where gradients diminish as they are backpropagated through time, making it challenging for the model to learn long-term dependencies.\n",
    "\n",
    "- **LSTM's Solution:** LSTMs address the vanishing gradient problem with a more complex architecture that includes memory cells and gating mechanisms. This allows LSTMs to selectively remember or forget information over long sequences, enabling them to capture dependencies over extended contexts.\n",
    "\n",
    "#### 3. Context Understanding in Toxicity Classification:\n",
    "\n",
    "- **Context Sensitivity:** Toxicity in language often involves subtle nuances and context-dependent meanings. A model must understand the context of words in a sentence to accurately identify toxic elements.\n",
    "\n",
    "- **Long-Term Dependencies:** LSTMs excel at capturing long-term dependencies, allowing them to consider the entire context of a sentence, even when the toxic elements are separated by several words.\n",
    "\n",
    "#### 4. Enhanced Semantic Understanding:\n",
    "\n",
    "- **Semantic Richness:** LSTMs, by virtue of their architecture, can generate embeddings that carry rich semantic information. This enables the model to learn not just from individual words but also from the relationships between them.\n",
    "\n",
    "#### 5. Adaptability to Variable-Length Sequences:\n",
    "\n",
    "- **Handling Varied Lengths:** Comments or sentences in toxicity classification datasets can have varying lengths. LSTMs can handle variable-length sequences through their ability to process input sequentially and produce a fixed-size output regardless of input length.\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "In summary, the choice of LSTM for toxicity classification is grounded in its ability to overcome the limitations of regular RNNs, capture long-term dependencies, understand context, and handle variable-length sequences. The sequential nature of language is effectively modeled, making LSTMs a suitable choice for tasks where the order of words significantly influences the meaning, as is the case in toxicity classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ebb9f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8831d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\huzai\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "from gpt4all import Embed4All\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053aae5e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c3530",
   "metadata": {},
   "source": [
    "### Preprocessing Approach Explanation:\n",
    "\n",
    "#### 1. Dataset Loading:\n",
    "\n",
    "- **Test and Train Data:** Two datasets, test_data and train_data, are loaded from CSV files (data/test.csv and data/train.csv respectively).\n",
    "- **Test Labels:** Test labels are loaded from a separate CSV file (data/test_labels.csv).\n",
    "\n",
    "#### 2. Filtering Train Data:\n",
    "\n",
    "- **Conditions:** The train_data is filtered based on specific conditions for each toxicity label. Rows are retained only if all toxicity labels (toxic, severe_toxic, obscene, threat, insult, identity_hate) have valid values (not equal to -1).\n",
    "\n",
    "#### 3. Text Normalization Functions:\n",
    "\n",
    "- **Lowercasing (lowercase):** Converts text to lowercase, ensuring uniformity and reducing the vocabulary size.\n",
    "- **Removing Punctuation (remove_punctuation):** Eliminates punctuation, reducing noise in the text.\n",
    "- **Removing Stopwords (remove_stopwords):** Removes common English stopwords, reducing dimensionality and focusing on informative words.\n",
    "- **Removing Numbers (remove_numbers):** Excludes numerical values, as they may not contribute significantly to toxicity classification.\n",
    "- **Removing URLs (remove_url):** Eliminates URLs, as they often do not contribute to the semantic meaning of the text.\n",
    "\n",
    "#### 4. Normalization Pipeline (normalize_sentence):\n",
    "\n",
    "- **Integration of Functions:** The normalize_sentence function integrates all the above preprocessing functions to provide a comprehensive text normalization pipeline.\n",
    "\n",
    "#### 5. Applying Normalization to Train Data:\n",
    "\n",
    "- **Normalization of Comment Texts:** The comment_text column in the train_data is transformed using the normalize_sentence function.\n",
    "- **Length Filter:** Rows are further filtered based on the length of the normalized comment text. Only comments with a length greater than 20 characters are retained.\n",
    "\n",
    "#### 6. Train Data and Labels:\n",
    "\n",
    "- **Conversion to NumPy Arrays:** The comment_text column is converted to a NumPy array for model input.\n",
    "- **Labels Extraction:** The toxicity labels are extracted from the filtered train_data and converted to a NumPy array for model training.\n",
    "\n",
    "### Why This Preprocessing Works:\n",
    "\n",
    "1. **Noise Reduction:**\n",
    "   - The combination of lowercase conversion, punctuation removal, and stopword elimination reduces noise in the text, helping the model focus on meaningful words.\n",
    "\n",
    "2. **Dimensionality Reduction:**\n",
    "   - Removing numbers and stopwords reduces the dimensionality of the data, making it more manageable and improving model efficiency.\n",
    "\n",
    "3. **Normalization for Consistency:**\n",
    "   - Normalizing the text ensures consistency in representation, allowing the model to learn more effectively across diverse comments.\n",
    "\n",
    "4. **Handling URLs:**\n",
    "   - Removing URLs is beneficial as they typically do not contribute much to the meaning of the text and can introduce noise.\n",
    "\n",
    "5. **Length Filtering:**\n",
    "   - Filtering out short comments (length < 20) ensures that the model is trained on comments with sufficient context, improving its ability to understand and classify toxicity in longer sentences.\n",
    "\n",
    "6. **Label Filtering:**\n",
    "   - Filtering train_data based on the availability of valid toxicity labels ensures that the model is trained on relevant and annotated data, avoiding potential issues with missing or incomplete labels.\n",
    "\n",
    "In summary, this preprocessing approach is designed to clean and standardize the text data, reduce noise, and focus on informative features. These steps aim to enhance the models ability to learn meaningful patterns and improve its performance in toxicity classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa41c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data = train_data[(train_data['toxic'] != -1) & (train_data['severe_toxic'] != -1) & (train_data['obscene'] != -1) & (train_data['threat'] != -1) & (train_data['insult'] != -1) & (train_data['identity_hate'] != -1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fad74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_labels = pd.read_csv('data/test_labels.csv')\n",
    "\n",
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data = train_data[(train_data['toxic'] != -1) & (train_data['severe_toxic'] != -1) & (train_data['obscene'] != -1) & (train_data['threat'] != -1) & (train_data['insult'] != -1) & (train_data['identity_hate'] != -1)]\n",
    "# nltk.download('stopwords')\n",
    "def lowercase(txt):\n",
    "    \n",
    "    return txt.lower()\n",
    "\n",
    "def remove_punctuation(txt):\n",
    "    txt=re.sub(r'[^\\w\\s]', '', txt)\n",
    "    return txt\n",
    "\n",
    "def remove_stopwords(txt):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    txt_parsed = txt.split()\n",
    "    removed=[x for x in txt_parsed if x not in stop_words]\n",
    "    txt=' '.join(removed)\n",
    "    return txt\n",
    "\n",
    "def remove_numbers(txt):\n",
    "    txt_parsed = txt.split()\n",
    "    removed=[x for x in txt_parsed if x.isalpha()]\n",
    "    txt=' '.join(removed)\n",
    "    return txt\n",
    "\n",
    "def remove_url(txt):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    txt = url.sub('',txt)\n",
    "    return txt\n",
    "\n",
    "def normalize_sentence(txt):\n",
    "    '''\n",
    "    Aggregates all the above functions to normalize/clean a sentence\n",
    "    '''\n",
    "    txt=lowercase(txt)\n",
    "    txt=remove_punctuation(txt)\n",
    "    txt=remove_stopwords(txt)\n",
    "    txt=remove_numbers(txt)\n",
    "    txt=remove_url(txt)\n",
    "    \n",
    "    return txt\n",
    "\n",
    "train_data['comment_text'] = train_data['comment_text'].apply(normalize_sentence)\n",
    "train_data = train_data[train_data['comment_text'].apply(lambda x: len(x) > 20)]\n",
    "\n",
    "train_labels = np.array(train_data[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n",
    "train_data = np.array(train_data[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a3a53a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explanation edits made username hardcore metallica fan reverted werent vandalisms closure gas voted new york dolls fac please dont remove template talk page since im retired'\n",
      " 'daww matches background colour im seemingly stuck thanks talk january utc'\n",
      " 'hey man im really trying edit war guy constantly removing relevant information talking edits instead talk page seems care formatting actual info'\n",
      " ... 'spitzer umm theres actual article prostitution ring crunch captain'\n",
      " 'looks like actually put speedy first version deleted look'\n",
      " 'really dont think understand came idea bad right away kind community goes bad ideas go away instead helping rewrite']\n"
     ]
    }
   ],
   "source": [
    "X = train_data\n",
    "y = train_labels\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # Define the maximum number of words in your vocabulary\n",
    "max_len = 100  # Define the maximum length of sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_val_padded = pad_sequences(X_val_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b27b4",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Parameter Choices:\n",
    "\n",
    "#### 1. Embedding Dimension (`embedding_dim = 100`):\n",
    "\n",
    "- **Decision:** The embedding dimension is set to 100.\n",
    "- **Explanation:** The embedding dimension determines the size of the word embeddings learned by the model. A lower value, such as 50, is chosen to reduce the dimensionality of the word representations. This is often a trade-off between capturing rich semantic information and managing model complexity and a less complex model was enough in this case. Higher dimensions actually increased the error and validation and test loss as per our experiments.\n",
    "\n",
    "#### 2. Model Architecture:\n",
    "\n",
    "- **Decision:** Sequential model architecture with an Embedding layer, an LSTM layer, and a Dense output layer.\n",
    "- **Explanation:** This architecture is suitable for sequential data processing, capturing long-term dependencies with LSTM, and producing a binary classification output with the Dense layer.\n",
    "\n",
    "#### 3. LSTM Layer Configuration (`LSTM(50)`):\n",
    "\n",
    "- **Decision:** LSTM layer with 50 units.\n",
    "- **Explanation:** The number of LSTM units determines the capacity of the layer to capture sequential patterns. A choice of 50 units strikes a balance between model capacity and avoiding overfitting.A higher value overfitted the data and increase test loss.\n",
    "\n",
    "#### 4. Dense Output Layer (`Dense(6, activation='sigmoid')`):\n",
    "\n",
    "- **Decision:** Dense output layer with 6 units (for each toxicity type) and sigmoid activation.\n",
    "- **Explanation:** The output layer is designed for binary classification of each toxicity type independently. Sigmoid activation squashes output values to the range [0, 1], allowing the model to predict the probability of the presence of each toxicity type.\n",
    "\n",
    "#### 5. Model Compilation:\n",
    "\n",
    "- **Decision:** Adam optimizer, binary cross-entropy loss, and accuracy metric.\n",
    "- **Explanation:** The Adam optimizer is chosen for its adaptive learning rate capabilities. Binary cross-entropy is suitable for binary classification tasks, and accuracy is chosen as the evaluation metric.\n",
    "\n",
    "#### 6. Training Parameters (`epochs = 4`, `batch_size = 64`):\n",
    "\n",
    "- **Decision:** Training for 4 epochs with a batch size of 64.\n",
    "- **Explanation:** The number of epochs determines how many times the model iterates over the entire training dataset. A lower value of 4 is chosen, which can be beneficial for faster training as a higher value vastly increased the computation time increased loss with some value of batch sizes. The batch size of 64 balances the trade-off between computational efficiency and stable gradient updates during training. A lower batch size overfitted the data and vastly increased computation time while their was no need for a higher one as 64 seemed enough to handle the complexity of the task\n",
    "\n",
    "#### 7. Model Training:\n",
    "\n",
    "- **Training Data and Validation Data:** The model is trained on `X_train_padded` and `y_train` with a validation split using `validation_data=(X_val_padded, y_val)`.\n",
    "\n",
    "#### 8. Evaluation:\n",
    "\n",
    "- **Validation Loss and Accuracy:** The model's performance is evaluated on the validation set, and the loss and accuracy metrics are printed.\n",
    "\n",
    "### Why These Choices:\n",
    "\n",
    "- **Embedding Dimension:**\n",
    "  - Lower embedding dimension reduces model complexity, potentially improving efficiency.\n",
    "\n",
    "- **LSTM Layer Units:**\n",
    "  - 50 LSTM units strike a balance between capturing sequential patterns and avoiding overfitting.\n",
    "\n",
    "- **Dense Output Layer:**\n",
    "  - 6 units with sigmoid activation are chosen for binary classification of each toxicity type independently.\n",
    "  - Sigmoid activation is suitable for binary classification tasks.\n",
    "\n",
    "- **Training Parameters:**\n",
    "  - 4 epochs are chosen to balance model learning and training time.\n",
    "  - Batch size of 64 balances computational efficiency and stable training dynamics.\n",
    "\n",
    "- **Model Compilation:**\n",
    "  - Adam optimizer adapts the learning rate during training.\n",
    "  - Binary cross-entropy loss suits binary classification.\n",
    "  - Accuracy is a suitable metric for binary classification evaluation.\n",
    "\n",
    "These choices aim to strike a balance between model complexity, computational efficiency, and the ability to capture sequential dependencies in text data. Fine-tuning hyperparameters and monitoring model performance during training and validation can further optimize these choices based on specific dataset characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361b079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # Choose the dimension of your word embeddings\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_len))\n",
    "model.add(LSTM(50))  # Adjust the number of LSTM units as needed\n",
    "model.add(Dense(6, activation='sigmoid'))  # Output layer with 6 units (for each toxicity type)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "829f0ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1883/1883 [==============================] - 95s 49ms/step - loss: 0.0756 - accuracy: 0.9812 - val_loss: 0.0531 - val_accuracy: 0.9940\n",
      "Epoch 2/4\n",
      "1883/1883 [==============================] - 87s 46ms/step - loss: 0.0473 - accuracy: 0.9937 - val_loss: 0.0525 - val_accuracy: 0.9943\n",
      "Epoch 3/4\n",
      "1883/1883 [==============================] - 85s 45ms/step - loss: 0.0419 - accuracy: 0.9935 - val_loss: 0.0508 - val_accuracy: 0.9940\n",
      "Epoch 4/4\n",
      "1883/1883 [==============================] - 89s 47ms/step - loss: 0.0368 - accuracy: 0.9832 - val_loss: 0.0539 - val_accuracy: 0.9724\n",
      "942/942 [==============================] - 10s 10ms/step - loss: 0.0539 - accuracy: 0.9724\n",
      "Validation Loss: 0.053878527134656906, Validation Accuracy: 0.972442626953125\n"
     ]
    }
   ],
   "source": [
    "epochs = 4  # Choose the number of epochs\n",
    "batch_size = 64  # Choose the batch size\n",
    "\n",
    "model.fit(X_train_padded, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val_padded, y_val))\n",
    "\n",
    "loss, accuracy = model.evaluate(X_val_padded, y_val)\n",
    "print(f'Validation Loss: {loss}, Validation Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27365899",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153159</th>\n",
       "      <td>fffcd0960ee309b5</td>\n",
       "      <td>. \\n i totally agree, this stuff is nothing bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153160</th>\n",
       "      <td>fffd7a9a6eb32c16</td>\n",
       "      <td>== Throw from out field to home plate. == \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153161</th>\n",
       "      <td>fffda9e8d6fafa9e</td>\n",
       "      <td>\" \\n\\n == Okinotorishima categories == \\n\\n I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153162</th>\n",
       "      <td>fffe8f1340a79fc2</td>\n",
       "      <td>\" \\n\\n == \"\"One of the founding nations of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153163</th>\n",
       "      <td>ffffce3fb183ee80</td>\n",
       "      <td>\" \\n :::Stop already. Your bullshit is not wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153164 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text\n",
       "0       00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1       0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2       00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3       00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4       00017695ad8997eb          I don't anonymously edit articles at all.\n",
       "...                  ...                                                ...\n",
       "153159  fffcd0960ee309b5  . \\n i totally agree, this stuff is nothing bu...\n",
       "153160  fffd7a9a6eb32c16  == Throw from out field to home plate. == \\n\\n...\n",
       "153161  fffda9e8d6fafa9e  \" \\n\\n == Okinotorishima categories == \\n\\n I ...\n",
       "153162  fffe8f1340a79fc2  \" \\n\\n == \"\"One of the founding nations of the...\n",
       "153163  ffffce3fb183ee80  \" \\n :::Stop already. Your bullshit is not wel...\n",
       "\n",
       "[153164 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beb73b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.concat([test_data, test_labels], axis=1)\n",
    "test_data = test_data[(test_data['toxic'] != -1) & (test_data['severe_toxic'] != -1) & (test_data['obscene'] != -1) & (test_data['threat'] != -1) & (test_data['insult'] != -1) & (test_data['identity_hate'] != -1)]\n",
    "\n",
    "\n",
    "test_data['comment_text'] = test_data['comment_text'].apply(normalize_sentence)\n",
    "test_data = test_data[test_data['comment_text'].apply(lambda x: len(x) > 20)]\n",
    "\n",
    "test_labels = np.array(test_data[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]])\n",
    "test_data = np.array(test_data[\"comment_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bd5e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b44cb405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e76638c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855/1855 [==============================] - 21s 11ms/step - loss: 0.0716 - accuracy: 0.9669\n",
      "Test Loss: 0.07164348661899567, Test Accuracy: 0.9669210910797119\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test_padded, test_labels)\n",
    "\n",
    "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990aac8",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24205b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942/942 [==============================] - 12s 12ms/step\n",
      "Classification Report on Validation Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.84      0.69      0.76      2805\n",
      " severe_toxic       0.57      0.14      0.23       277\n",
      "      obscene       0.84      0.74      0.79      1513\n",
      "       threat       1.00      0.04      0.09        90\n",
      "       insult       0.74      0.58      0.65      1429\n",
      "identity_hate       0.61      0.16      0.25       276\n",
      "\n",
      "    micro avg       0.81      0.62      0.71      6390\n",
      "    macro avg       0.77      0.39      0.46      6390\n",
      " weighted avg       0.80      0.62      0.69      6390\n",
      "  samples avg       0.06      0.06      0.06      6390\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huzai\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\huzai\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get model predictions on validation set\n",
    "y_val_pred_prob = model.predict(X_val_padded)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "y_val_pred = (y_val_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate classification report\n",
    "class_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "classification_rep = classification_report(y_val, y_val_pred, target_names=class_names)\n",
    "\n",
    "print(\"Classification Report on Validation Set:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "295bfd35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855/1855 [==============================] - 26s 14ms/step\n",
      "Classification Report on Test Set:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "        toxic       0.56      0.78      0.65      5137\n",
      " severe_toxic       0.42      0.26      0.32       337\n",
      "      obscene       0.61      0.72      0.66      3149\n",
      "       threat       0.27      0.02      0.04       178\n",
      "       insult       0.64      0.58      0.61      2942\n",
      "identity_hate       0.63      0.18      0.28       608\n",
      "\n",
      "    micro avg       0.59      0.66      0.62     12351\n",
      "    macro avg       0.52      0.42      0.43     12351\n",
      " weighted avg       0.59      0.66      0.61     12351\n",
      "  samples avg       0.06      0.06      0.06     12351\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\huzai\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\huzai\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Get model predictions on test set\n",
    "y_test_pred_prob = model.predict(X_test_padded)\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "y_test_pred = (y_test_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Generate classification report\n",
    "class_names = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "classification_rep_test = classification_report(test_labels, y_test_pred, target_names=class_names)\n",
    "\n",
    "print(\"Classification Report on Test Set:\\n\", classification_rep_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6a5910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
